{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187effbf",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "This is an application of Confidence Intervals and Hypotheses Testing.\n",
    "\n",
    "The A/B Testing is a comparison between two groups (A and B), as illustrated in Figure 1.\n",
    "\n",
    "![](imgs/c4_l13_01.png)\n",
    "\n",
    "<center><em>Figure 1 - Two group to be tested.</em></center>\n",
    "\n",
    ">A/B tests are used to test changes on a web page by running an experiment where a control group sees the old version, while the experiment group sees the new version. A metric is then chosen to measure the level of engagement from users in each group. These results are then used to judge whether one version is more effective than the other. A/B testing is very much like hypothesis testing with the following hypotheses:\n",
    "\n",
    "$$H_0 : \\text{The new version is equal or worse than the older version.} \\\\\n",
    " H_1 : \\text{The new version is better than the older version.} $$\n",
    "\n",
    "Decision:\n",
    "\n",
    "* If we fail to reject the null hypothesis, the results would suggest keeping the old version, or;\n",
    "* If we reject the null hypothesis, the results would suggest launching the change.\n",
    "\n",
    "#### Drawbacks {-}\n",
    "\n",
    ">It can help you compare two options, but it can't tell you about an option you haven't considered. It can also produce bias results when tested on existing users, due to factors like change aversion and novelty effect.\n",
    "\n",
    ">* Change Aversion: Existing users may give an unfair advantage to the old version, simply because they are unhappy with change, even if it's ultimately for the better.\n",
    ">* Novelty Effect: Existing users may give an unfair advantage to the new version, because they're excited or drawn to the change, even if it isn't any better in the long run.\n",
    "\n",
    "### Example: New Homepage\n",
    "\n",
    "The Audacity company want to perform an A/B Testing of two versions of a new homepage.\n",
    "\n",
    "$$H_0 : CTR_{new} - CTR_{old} \\leq 0 \\\\\n",
    "  H_1 : CTR_{new} - CTR_{old} > 0$$\n",
    "\n",
    "Where CTR stands to Click Through Rate.\n",
    "\n",
    "There are two version: `control` and `experiment`.\n",
    "\n",
    "The difference between the CTR is about 0.03.\n",
    "\n",
    "#### Bootstrapping {-}\n",
    "\n",
    "**Example:** New version of Home page.\n",
    "\n",
    "The bootstrapping provide a histogram presented in Figure 2.\n",
    "\n",
    "![](imgs/c4_l13_02.png)\n",
    "\n",
    "The null hypothesis histogram is showed in Figure 3.\n",
    "\n",
    "![](imgs/c4_l13_03.png)\n",
    "\n",
    "Finally, Figure 4 illustrate both histogram.\n",
    "\n",
    "![](imgs/c4_l13_04.png)\n",
    "\n",
    "#### P-value {-}\n",
    "\n",
    "Founded on the entire population (excepting the duplicated user id, etc.), I have calculated the `diff`.\n",
    "\n",
    "```\n",
    "diff = experiment_ctr - control_ctr = 0.030034443684015644\n",
    "```\n",
    "\n",
    "The `diff` could be interpreted as a threshold which I will use as delimiter, to do it I will calculate the proportion of $H_0$ (orange graph) that has a difference between CTR's higher than `diff`.\n",
    "\n",
    "For this reason, I will calculate the average of a list of `bool`, which will return the proportion I want.\n",
    "\n",
    "Based on the `p_value` of 0.5% we reject the $H_0$.\n",
    "\n",
    ">**Conclusion:** Audacity should launch the new version of the home page.\n",
    "\n",
    "### Example: Average Reading Time\n",
    "\n",
    "Same idea, two version of a website, one `control` and other `experiment`.\n",
    "\n",
    "* Average Reading time of control: 115.38637100678429\n",
    "* Average Reading time of experiment: 131.3208410471793\n",
    "* Difference observed: 15.9\n",
    "\n",
    "On average, visitor using the experiment version of website spent almost 16 more seconds.\n",
    "\n",
    "Hypotheses posed:\n",
    "\n",
    "$$H_0 : ART_{new} - ART_{old} \\leq 0 \\\\\n",
    "  H_1 : ART_{new} - ART_{old} > 0$$\n",
    "\n",
    "Where ART stands to Average Reading Time.\n",
    "\n",
    "#### Bootstrapping {-}\n",
    "\n",
    "Let's apply the bootstrapping, and plot a histogram in Figure 5.\n",
    "\n",
    "![](imgs/c4_l13_05.png)\n",
    "\n",
    "The null hypothesis histogram is showed in Figure 6.\n",
    "\n",
    "![](imgs/c4_l13_06.png)\n",
    "\n",
    "Finally, Figure 7 illustrate both histogram.\n",
    "\n",
    "![](imgs/c4_l13_07.png)\n",
    "\n",
    "#### P-value {-}\n",
    "\n",
    "The `p_value` is zero.\n",
    "\n",
    ">**Conclusion:** Reject the $H_0$ because p_value < $\\alpha$\n",
    "\n",
    "Where $\\alpha$ is 0.05.\n",
    "\n",
    "### Example: Enrollment Rate\n",
    "\n",
    "This is an example to show a case where the $H_0$ is failed to reject.\n",
    "\n",
    "I will use the same principle of CTR to evaluate the Enrollment rate.\n",
    "\n",
    "* Enrollment rate control: 0.23452157598499063\n",
    "* Enrollment rate experiment: 0.2642986152919928\n",
    "* Difference observed: 0.02977703930700215\n",
    "\n",
    "Hypotheses posed:\n",
    "\n",
    "$$H_0 : CTR_{new} - CTR_{old} \\leq 0 \\\\\n",
    "  H_1 : CTR_{new} - CTR_{old} > 0$$\n",
    "\n",
    "Where ER stands to Enrollment Rate.\n",
    "\n",
    "#### Bootstrapping {-}\n",
    "\n",
    "Let's apply the bootstrapping, and plot a histogram in Figure 8.\n",
    "\n",
    "![](imgs/c4_l13_08.png)\n",
    "\n",
    "The null hypothesis histogram is showed in Figure 9.\n",
    "\n",
    "![](imgs/c4_l13_09.png)\n",
    "\n",
    "Finally, Figure 10 illustrate both histogram.\n",
    "\n",
    "![](imgs/c4_l13_10.png)\n",
    "\n",
    "#### P-value {-}\n",
    "\n",
    "The `p_value` is 0.0624.\n",
    "\n",
    ">**Conclusion:** Due to p_value > $\\alpha$ we fail to reject $H_0$.\n",
    "\n",
    "Where $\\alpha$ is 0.05.\n",
    "\n",
    "### Bonferroni Correction\n",
    "\n",
    ">If you remember from the previous lesson, the Bonferroni Correction is one way we could handle experiments with multiple tests, or metrics in this case. To compute the new bonferroni correct alpha value, we need to divide the original alpha value by the number of tests.\n",
    "\n",
    "The new $\\alpha$ will be:\n",
    "\n",
    "$$\\alpha_{adjusted} = \\frac{\\alpha}{4} = \\frac{0.05}{4} = 0.0125$$\n",
    "\n",
    "Based on the several test we have done:\n",
    "\n",
    "* Enrollment Rate: 0.0624 (Read the Jupyther Notebook)\n",
    "* Average Reading Duration: 0 (Read the Jupyther Notebook)\n",
    "* Average Classroom Time: 0.0384 (Read the Jupyther Notebook)\n",
    "* Completion Rate: 0.0846 (Read the Jupyther Notebook)\n",
    "\n",
    "This new $\\alpha$ will generate only **one** A/B Testing statistical significant.\n",
    "\n",
    "|New Feature|p value|$\\alpha_{adjusted}$|Result|$H_0$|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|Enrollment Rate|0.0624|0.0125|>|Fail to reject $H_0$|\n",
    "|Average Reading Duration|0|0.0125|<|<strong>Reject $H_0$</strong>|\n",
    "|Average Classroom Time|0.0384|0.0125|>|Fail to reject $H_0$|\n",
    "|Completion Rate|0.0846|0.0125|>|Fail to reject $H_0$|\n",
    "\n",
    "This is the reason the Bonferroni method is considered conservative.\n",
    "\n",
    "### Difficulties in A/B Testing\n",
    "\n",
    ">As you saw in the scenarios above, there are many factors to consider when designing an A/B test and drawing conclusions based on its results. To conclude, here are some common ones to consider.\n",
    "\n",
    ">* Novelty effect and change aversion when existing users first experience a change\n",
    ">* Sufficient traffic and conversions to have significant and repeatable results\n",
    ">* Best metric choice for making the ultimate decision (e.g. measuring revenue vs. clicks)\n",
    ">* Long enough run time for the experiment to account for changes in behavior based on time of day/week or seasonal events.\n",
    ">* Practical significance of a conversion rate (the cost of launching a new feature vs. the gain from the increase in conversion)\n",
    ">* Consistency among test subjects in the control and experiment group (imbalance in the population represented in each group can lead to situations like Simpson's Paradox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c994ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
